{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf8bd14-1316-4029-a2a9-70efbb50ed2d",
   "metadata": {},
   "source": [
    "## Log Classification Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7549c838-0180-4d94-aa62-aafd3e443843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Embedding\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ea7b7-f779-4725-958b-bc7f681ea91c",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b67acdc-c569-4a3f-9a50-26c332dffc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.19.102:54106 dest: /10.250.19.102:50010</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar.</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.10.6:40524 dest: /10.250.10.6:50010</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.14.224:42420 dest: /10.250.14.224:50010</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block  terminating</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   Log  \\\n",
       "0             143 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.19.102:54106 dest: /10.250.19.102:50010   \n",
       "1  35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar.   \n",
       "2                 143 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.10.6:40524 dest: /10.250.10.6:50010   \n",
       "3             145 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.14.224:42420 dest: /10.250.14.224:50010   \n",
       "4                                      145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block  terminating   \n",
       "\n",
       "    Label  \n",
       "0  Normal  \n",
       "1  Normal  \n",
       "2  Normal  \n",
       "3  Normal  \n",
       "4  Normal  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./combined_logs_with_labels.csv',delimiter=',', encoding='latin-1')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd57fee-0caf-44a4-afd1-9c7c6352635a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "437a3f85-aae8-4c7a-a2e2-cc24a98abef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making it TF.Data and spliting into 2 datasets\n",
    "df['Label'] = df['Label'].map({'Normal':0,'Anomaly':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094af48f-689c-4ad8-a16f-c46e5239d4bb",
   "metadata": {},
   "source": [
    "##### Train-test split ensuring 80% of the anomolous data is within the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7b70d10-c13f-4a55-811e-ff4853bf9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the entire DataFrame\n",
    "df = df.sample(frac=1, random_state=38).reset_index(drop=True)  # Shuffle the entire dataset\n",
    "\n",
    "# Total samples\n",
    "total_samples = len(df)\n",
    "train_size = int(total_samples * 0.8)  # 80% of the entire dataset for training\n",
    "\n",
    "# Separate the classes\n",
    "normal_samples = df[df['Label'] == 0]\n",
    "anomaly_samples = df[df['Label'] == 1]\n",
    "\n",
    "#print(total_samples) # 19412\n",
    "#print(train_size) # 15529\n",
    "#print(len(normal_samples)) # 17220\n",
    "#print(len(anomaly_samples)) # 2192\n",
    "\n",
    "# Determine the number of Anomaly samples for the training set\n",
    "anomaly_train_size = int(len(anomaly_samples) * 0.8)  # 80% of Anomaly samples\n",
    "normal_train_size = train_size - anomaly_train_size  # Remaining from Normals\n",
    "\n",
    "# Sample from each class\n",
    "normal_train = normal_samples.sample(normal_train_size, random_state=38)\n",
    "anomaly_train = anomaly_samples.sample(anomaly_train_size, random_state=38)\n",
    "#print(len(normal_train)) # 13776\n",
    "#print(len(anomaly_train)) # 1753\n",
    "\n",
    "# Combine the training samples\n",
    "train_df = pd.concat([anomaly_train, normal_train])\n",
    "# Remaining as test samples\n",
    "test_df = pd.concat([anomaly_samples, normal_samples]).drop(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be7dc16-6cd5-4586-afe4-6a2fb085997c",
   "metadata": {},
   "source": [
    "##### Handling class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c00c4dfc-a1b8-422d-a01f-d902e3b30863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.56\n",
      "Weight for class 1: 4.43\n"
     ]
    }
   ],
   "source": [
    "normal = train_df['Label'].value_counts()[0]\n",
    "anomaly = train_df['Label'].value_counts()[1]\n",
    "#normal, anomaly\n",
    "total = normal + anomaly\n",
    "weight_for_0 = (1 / normal) * (total) / 2.0\n",
    "weight_for_1 = (1 / anomaly) * (total) / 2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1} #create a dictionary\n",
    "\n",
    "print(\"Weight for class 0: {:.2f}\".format(weight_for_0))\n",
    "print(\"Weight for class 1: {:.2f}\".format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c25a8939-0003-4cbe-b6df-413132a4abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['Log'].values, train_df['Label'].values))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(df), seed=seed)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df['Log'].values, test_df['Label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4dbb679-0c47-44e4-8665-8957dc2b4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of words in your vocabulary\n",
    "max_words = 10000\n",
    "# Define the sequence length\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1ed65-0b4f-4aab-8855-055354f64345",
   "metadata": {},
   "source": [
    "##### Text Vectorization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c131da66-183c-4e61-b186-b71f9a709779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=max_words,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a88905f-c1f4-46c7-93c1-b52a1760ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the layer to your training data (this builds the vocabulary)\n",
    "text_ds = train_dataset.map(lambda x, y: x) #throw away y\n",
    "vectorize_layer.adapt(text_ds.batch(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b22330a-302e-4968-aeaa-56b8385381b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch & prefetch the dataset\n",
    "train_dataset = train_dataset.batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107dd81-b557-4f5e-b3c4-38be05cc0a13",
   "metadata": {},
   "source": [
    "##### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b9a6f66-aa32-4c50-a460-db9d4ca09a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Inp = Input(shape=(1,),dtype=tf.string, name='text_input')\n",
    "x = vectorize_layer(Inp)\n",
    "x = Embedding(max_words, 48, input_length=max_len, name=\"embedding\")(x) #48 is the representation\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x) # Max pooling over time\n",
    "x = Dense(128, activation='relu')(x)\n",
    "out = Dense(1, activation='sigmoid', name='output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aac9e079-5a01-4e8c-b3da-e496baca9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=Inp,outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fc1af0f-96c7-42de-8eb7-007644a21f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13130599-d933-4b92-96e3-0430edd91fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)\n",
    "#prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f09b31a1-3af4-4433-acd8-9d90de677403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.7819 - loss: 0.5643 - val_accuracy: 0.9464 - val_loss: 0.1810\n",
      "Epoch 2/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 70ms/step - accuracy: 0.9158 - loss: 0.2328 - val_accuracy: 0.9583 - val_loss: 0.1224\n",
      "Epoch 3/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 70ms/step - accuracy: 0.9510 - loss: 0.1972 - val_accuracy: 0.9583 - val_loss: 0.1718\n",
      "Epoch 4/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 70ms/step - accuracy: 0.9614 - loss: 0.1772 - val_accuracy: 0.9629 - val_loss: 0.1629\n",
      "Epoch 5/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 71ms/step - accuracy: 0.9613 - loss: 0.1693 - val_accuracy: 0.9639 - val_loss: 0.1568\n",
      "Epoch 6/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 70ms/step - accuracy: 0.9627 - loss: 0.1706 - val_accuracy: 0.9598 - val_loss: 0.1726\n",
      "Epoch 7/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 70ms/step - accuracy: 0.9621 - loss: 0.1621 - val_accuracy: 0.9562 - val_loss: 0.1708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21aaafe1820>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset,\n",
    "          epochs=10,\n",
    "          validation_data=test_dataset,\n",
    "          class_weight = class_weight,\n",
    "          callbacks=[early_stopping]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df51143d-9e9b-4118-99d8-3b18f46ec356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9313 - loss: 0.1704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1567613035440445, 0.9639453887939453]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292d45e-9b7a-4473-b5e8-ce04b8affab0",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "646b4beb-a639-4631-aea9-e3ef98322d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00014648]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This log event is supposed to be Normal\n",
    "text = ['INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.102:50010 is added to size 67108864']\n",
    "\n",
    "model.predict(tf.constant(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e6077df-74c5-4eb8-a15b-47d9c90c7dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9231655]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This log event is supposed to be an Anomaly\n",
    "text = ['29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000074_2/part-00074.']\n",
    "\n",
    "model.predict(tf.constant(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf7801-c559-4c36-9337-3724b13fb024",
   "metadata": {},
   "source": [
    "##### From the results, CNN does better than LSTM in the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6347bc-d3e4-4f9d-bc77-7b263cde07e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
