{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acc10b7-9bb5-4796-a4c5-866889739cc4",
   "metadata": {},
   "source": [
    "## Log Classification Using Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb592998-0f35-485c-8dae-9ff0f0b8e856",
   "metadata": {},
   "source": [
    "##### Codes were mainly referenced from https://www.analyticsvidhya.com/blog/2024/06/finetuning-llama-3-for-sequence-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96bcfb0-6d10-40a8-88e9-d45d8367ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44.1\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d11337b-542a-4bf6-b5fe-3718b683c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers accelerate trl bitsandbytes datasets evaluate\n",
    "#!pip install -q peft scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7549c838-0180-4d94-aa62-aafd3e443843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "903df680-ead8-4cf2-9edf-1aa77d1c8100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b2a7c-b87c-45c1-9f76-ecc808eb7b00",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b67acdc-c569-4a3f-9a50-26c332dffc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.19.102:54106 dest: /10.250.19.102:50010</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar.</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.10.6:40524 dest: /10.250.10.6:50010</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.14.224:42420 dest: /10.250.14.224:50010</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block  terminating</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   Log  \\\n",
       "0             143 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.19.102:54106 dest: /10.250.19.102:50010   \n",
       "1  35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar.   \n",
       "2                 143 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.10.6:40524 dest: /10.250.10.6:50010   \n",
       "3             145 INFO dfs.DataNode$DataXceiver: Receiving block  src: /10.250.14.224:42420 dest: /10.250.14.224:50010   \n",
       "4                                      145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block  terminating   \n",
       "\n",
       "    Label  \n",
       "0  Normal  \n",
       "1  Normal  \n",
       "2  Normal  \n",
       "3  Normal  \n",
       "4  Normal  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./combined_logs_with_labels.csv',delimiter=',', encoding='latin-1')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52485b-dbf8-4aaf-ad03-3574f87e3f80",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7d714b-d515-48f7-8076-8e806e16f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making it TF.Data and spliting into 2 datasets\n",
    "df['Label'] = df['Label'].map({'Normal':0,'Anomaly':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c134b7-de8a-4d21-b152-95d0d0cc23a2",
   "metadata": {},
   "source": [
    "##### Train-test split ensuring 80% of the anomolous data is within the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc11e14b-65c2-40ba-8a10-2872d7e4c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the entire DataFrame\n",
    "df = df.sample(frac=1, random_state=38).reset_index(drop=True)  # Shuffle the entire dataset\n",
    "\n",
    "# Total samples\n",
    "total_samples = len(df)\n",
    "train_size = int(total_samples * 0.8)  # 80% of the entire dataset for training\n",
    "\n",
    "# Separate the classes\n",
    "normal_samples = df[df['Label'] == 0]\n",
    "anomaly_samples = df[df['Label'] == 1]\n",
    "\n",
    "#print(total_samples) # 19412\n",
    "#print(train_size) # 15529\n",
    "#print(len(normal_samples)) # 17220\n",
    "#print(len(anomaly_samples)) # 2192\n",
    "\n",
    "# Determine the number of Anomaly samples for the training set\n",
    "anomaly_train_size = int(len(anomaly_samples) * 0.8)  # 80% of Anomaly samples\n",
    "normal_train_size = train_size - anomaly_train_size  # Remaining from Normals\n",
    "\n",
    "# Sample from each class\n",
    "normal_train = normal_samples.sample(normal_train_size, random_state=38)\n",
    "anomaly_train = anomaly_samples.sample(anomaly_train_size, random_state=38)\n",
    "#print(len(normal_train)) # 13776\n",
    "#print(len(anomaly_train)) # 1753\n",
    "\n",
    "# Combine the training samples\n",
    "train_df = pd.concat([anomaly_train, normal_train])\n",
    "# Remaining as test samples\n",
    "test_df = pd.concat([anomaly_samples, normal_samples]).drop(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d5aa7-3a62-4940-8479-d73ad5e51a71",
   "metadata": {},
   "source": [
    "##### Converting pandas DataFrames into Hugging Face Dataset objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf70128a-3120-43dc-8e77-1a0624467ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "# dataset_val = Dataset.from_pandas(val_df)\n",
    "dataset_test = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine them into a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    #'validation': dataset_val,\n",
    "    'test': dataset_test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89d4b6-040e-4494-aa55-7ec38defc773",
   "metadata": {},
   "source": [
    "##### Class weights for handling class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "417ce42e-a459-4ec3-a06a-0aa744add2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.56\n",
      "Weight for class 1: 4.43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normal = train_df['Label'].value_counts()[0]\n",
    "anomaly = train_df['Label'].value_counts()[1]\n",
    "total = normal + anomaly\n",
    "weight_for_0 = (1 / normal) * (total) / 2.0\n",
    "weight_for_1 = (1 / anomaly) * (total) / 2.0\n",
    "\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}  # Create a dictionary\n",
    "\n",
    "print(\"Weight for class 0: {:.2f}\".format(weight_for_0))\n",
    "print(\"Weight for class 1: {:.2f}\".format(weight_for_1))\n",
    "\n",
    "import torch\n",
    "\n",
    "class_weights_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float32)\n",
    "#class_weights = (1/train_df['Label'].value_counts(normalize=True).sort_index()).tolist()\n",
    "#class_weights = torch.tensor(class_weights).to(device)  # Move class weights to the GPU\n",
    "#class_weights = class_weights / class_weights.sum()\n",
    "#class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bf950-522b-4ddc-8656-6e20f060563c",
   "metadata": {},
   "source": [
    "##### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0073d671-1383-4e72-924c-d6d16614fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7207b516-a38a-480e-8da2-a0e759494653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a padding token \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Using eos_token as pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fff9a-c9c3-4023-bc01-91e90bd30745",
   "metadata": {},
   "source": [
    "##### Data preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec40bd63-02d9-4ca6-92a9-725bb64f424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(batch):\n",
    "    # Tokenize the logs and include the labels\n",
    "    tokenized = tokenizer(batch['Log'], truncation=True, max_length=512, padding=True)\n",
    "    tokenized['labels'] = batch['Label']  # Add the labels to the tokenized output\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e9b9a-5fb7-4875-b9ab-685dfcdb6f9f",
   "metadata": {},
   "source": [
    "##### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf559fab-5bd1-4a44-96bf-cf60f812513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18e7e87e3ed46a197f7006ceb029606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169b7e056f1344288d45f3aebb67987b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = dataset.map(data_preprocessing, batched=True, remove_columns=['Log', 'Label'])\n",
    "tokenized_data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900e9a3-1de5-49a8-b3b0-0c50fefab3ea",
   "metadata": {},
   "source": [
    "##### Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23908a8d-4cb1-47a3-9b8f-eb512caa7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b7df5-29cd-4507-9c55-de3ddd9c154b",
   "metadata": {},
   "source": [
    "##### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bad3ce1a-fd69-4970-ad49-4037ef4849b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model's configuration also recognizes the pad token\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=2,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22f3c3-4d15-43d3-9c70-7949f6361368",
   "metadata": {},
   "source": [
    "##### Define a custom Trainer class to incorporate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36ace585-1d82-439e-9681-4cbf2bab878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Forward pass\n",
    "        labels = inputs.get(\"labels\").to(device)  # Move labels to GPU\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Calculate the weighted loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc01a2-a995-4717-84ae-d641b6e5391a",
   "metadata": {},
   "source": [
    "##### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f045cc2c-fefe-48ec-9c6a-d494d07a0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6950a-6b3e-404a-bc82-a276e86b7982",
   "metadata": {},
   "source": [
    "##### Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fcc9291-564e-46fe-8c9f-5b78e4c4978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                 # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499f36b-edd4-4561-972d-01adad995fd5",
   "metadata": {},
   "source": [
    "##### Initialize custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62356347-c400-4d7c-b9cc-3ed20bfb1909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10adm\\AppData\\Local\\Temp\\ipykernel_19660\\3604839304.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['test'],  \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=class_weights_tensor.to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cc4c2-e055-4882-8cfc-39641c59a828",
   "metadata": {},
   "source": [
    "##### Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "280cfc33-2e75-4f87-a4de-4bbcc3a302ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1942' max='1942' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1942/1942 01:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>0.217304</td>\n",
       "      <td>0.942570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1942, training_loss=0.6529541709733673, metrics={'train_runtime': 100.4602, 'train_samples_per_second': 154.579, 'train_steps_per_second': 19.331, 'total_flos': 299278522765896.0, 'train_loss': 0.6529541709733673, 'epoch': 1.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac42428-e030-4bf2-b323-6c4b5a04624e",
   "metadata": {},
   "source": [
    "##### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30bf9475-e08f-46fd-bfab-775124be464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./transformer_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97147506-289a-400b-b2a1-8dc3b219d933",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abdc6c79-c589-48d9-b12c-d794aca26c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",  \n",
    "    model=\"./transformer_model\",\n",
    "    tokenizer=\"./transformer_model\",\n",
    "    device_map=\"cuda\", #cuda for gpt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3acbf634-01c2-4390-92a1-73e716afbc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9820976853370667}]\n"
     ]
    }
   ],
   "source": [
    "# This is supposed to be a Normal log\n",
    "text = ['INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.102:50010 is added to size 67108864']\n",
    "\n",
    "predictions = classifier(text)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "06b13682-35de-417a-a71b-fdf072eb20fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9505198001861572}]\n"
     ]
    }
   ],
   "source": [
    "# This is supposed to be an Anomaly log\n",
    "text = ['INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000074_2/part-00074.']\n",
    "\n",
    "predictions = classifier(text)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c5247-1008-46a6-b7cb-8f9efd9ed09c",
   "metadata": {},
   "source": [
    "##### Although the accuracy for transformer is high, it does not seemed to work well on unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2ef85-8fb8-47a2-a7ca-b047420695db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d280e0-3b5f-4711-926c-105c10b45787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a84b4e-1f76-4209-99ae-0959dd5e5d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6347bc-d3e4-4f9d-bc77-7b263cde07e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
